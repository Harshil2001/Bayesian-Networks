# -*- coding: utf-8 -*-
"""Copy of MIXTURE_CLT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mV53KYnq5UZro-d_zvN9kGIrlshKA3sw
"""

# # Unzipping the datasets if needed            
# !unzip /content/datasets.zip -d /content/

from __future__ import print_function
import numpy as np
import sys
import time
from Util import *
import os
from CLT_class import CLT
from collections import defaultdict

class MIXTURE_CLT():
    
    def __init__(self):
        self.n_components = 0 # number of components
        self.mixture_probs = None # mixture probabilities
        self.clt_list =[]   # List of Tree Bayesian networks
        

    '''
        Learn Mixtures of Trees using the EM algorithm.
    '''
    def learn(self, dataset, n_components=2, max_iter=50, epsilon=1e-5):
        self.n_components = n_components
        # For each component and each data point, we have a weight
        weights=np.zeros((n_components,dataset.shape[0]))

        #initialize mixture probs
        self.mixture_probs=np.zeros(n_components)
        # Randomly initialize the chow-liu trees and the mixture probabilities
        # Your code for random initialization goes here

        X = np.random.randint(low=2, high=200, size=(n_components,dataset.shape[0]))  # (No of data points ,k)
        a = []
        for i in X:
            i = i/np.sum(i)
            a.append(i)
            norm = np.asarray(a)
            self.mixture_probs = norm  # LAMBDA?

        tempwt = np.zeros((n_components, dataset.shape[0]))
        self.clt_list = [CLT() for i in range(n_components)]
        for k in range(n_components):
            self.clt_list[k].learn(dataset)

        LL = 0

        for itr in range(max_iter):
            #E-step: Complete the dataset to yield a weighted dataset
            # We store the weights in an array weights[ncomponents,number of points]
            
            #Your code for E-step here
            
            # M-step: Update the Chow-Liu Trees and the mixture probabilities
            
            #Your code for M-Step here
            for k in range(n_components):
                for i, sample in enumerate(dataset):
                    tempwt[k][i] = self.clt_list[k].getProb(sample)
                weights[k] = np.multiply(self.mixture_probs[k], tempwt[k])/np.sum(np.multiply(self.mixture_probs[k], tempwt[k]))
            for k in range(n_components):
                self.clt_list[k].update(dataset,weights[k])

            if(itr == 0):
                LL = mix.computeLL(dataset)
                print(LL)
            else:
                firstLL = mix.computeLL(dataset)
                print(firstLL)
                LL = firstLL
        
    
    """
        Compute the log-likelihood score of the dataset
    """
    def computeLL(self, dataset):
        ll = 0.0
        # Write your code below to compute likelihood of data
        #   Hint:   Likelihood of a data point "x" is sum_{c} P(c) T(x|c)
        #           where P(c) is mixture_prob of cth component and T(x|c) is the probability w.r.t. chow-liu tree at c
        #           To compute T(x|c) you can use the function given in class CLT
        llTemp = 0
        for i in range(dataset.shape[0]):
            for k in range(self.n_components):
                
                llTemp += np.multiply(self.mixture_probs[k], self.clt_list[k].getProb(dataset[i]))
            ll += np.log(llTemp)
        return ll/dataset.shape[0]
    

    
'''
    After you implement the functions learn and computeLL, you can learn a mixture of trees using
    To learn Chow-Liu trees, you can use
    mix_clt=MIXTURE_CLT()
    ncomponents=10 #number of components
    max_iter=50 #max number of iterations for EM
    epsilon=1e-1 #converge if the difference in the log-likelihods between two iterations is smaller 1e-1
    dataset=Util.load_dataset(path-of-the-file)
    mix_clt.learn(dataset,ncomponents,max_iter,epsilon)
    
    To compute average log likelihood of a dataset w.r.t. the mixture, you can use
    mix_clt.computeLL(dataset)/dataset.shape[0]
'''

c = 0
index = defaultdict(list)
file_path="/content/dataset/"
for i, file in enumerate(sorted(os.listdir(file_path))):
    if(i % 3 == 0):
        c += 1
    index[c].append(file)

# # VALIDATION - uncomment the next 20 lines for tuning the value of the hidden variable k
# for i in index:
#     print("=================================================================================\n")
#     print("FOR Dataset ",i,"\n\n")
#     dataset = Util.load_dataset("/content/dataset/"+index[i][1])
#     validateset = Util.load_dataset("/content/dataset/"+index[i][2])
#     testset = Util.load_dataset("/content/dataset/"+index[i][0])
#     mix = MIXTURE_CLT()

#     iter=0
#     for k in [2, 5, 10, 20]:
#         iter+=1
#         mix.learn(dataset, n_components=k, max_iter=5, epsilon=1e-3)
#         ll_list = mix.computeLL(validateset)
#         print("When k=", k, "Iteration:",iter," Log likelihood = ", ll_list,"\n")
#         ll_listt=np.array(ll_list)
#         mean=np.sum(ll_listt)/len(ll_listt)
#         print("\nMean: ", mean)

#     print("-----------------------------")

kTest = [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]
hiddenVariableList = defaultdict(int)
i = 0
for key, values in index.items():
    hiddenVariableList[key] = kTest[i]
    i += 1
for i in index:
    print(i)
    print("=================================================================================\n")
    print("FOR Dataset ",i,"\n\n")
    dataset = Util.load_dataset("/content/dataset/"+index[i][1])
    validateset = Util.load_dataset("/content/dataset/"+index[i][2])
    testset = Util.load_dataset("/content/dataset/"+index[i][0])
    mix = MIXTURE_CLT()
    for it in range(2):     ##Randomized.. So.. 5 times
        mix.learn(dataset, n_components=hiddenVariableList[int(i)], max_iter=3, epsilon=1e-3)
        print("when k =", hiddenVariableList[int(i)], "::",it+1,"th time")
        print("\nLL = ", mix.computeLL(testset))
        print("================")